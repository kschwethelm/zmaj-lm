{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mask Exploration\n",
    "\n",
    "This notebook demonstrates the various attention mask utilities and their visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from zmaj_lm.utils.masks import (\n",
    "    create_block_diagonal_mask,\n",
    "    create_causal_mask,\n",
    "    create_decoder_mask,\n",
    "    create_padding_mask,\n",
    "    create_sliding_window_mask,\n",
    "    mask_to_bias,\n",
    ")\n",
    "from zmaj_lm.utils.visualize import plot_attention_mask, plot_mask_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Causal Mask\n",
    "\n",
    "Prevents attention to future positions (lower triangular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 16\n",
    "causal = create_causal_mask(seq_len, device=\"cpu\")\n",
    "fig = plot_attention_mask(causal, title=\"Causal Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Padding Mask\n",
    "\n",
    "Masks out padding tokens in sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of 4 sequences with different lengths\n",
    "lengths = torch.tensor([16, 12, 8, 14])\n",
    "max_len = 16\n",
    "padding = create_padding_mask(lengths, max_len)\n",
    "\n",
    "print(f\"Padding mask shape: {padding.shape}\")\n",
    "print(f\"Lengths: {lengths.tolist()}\")\n",
    "\n",
    "# Combine padding with causal mask for decoder attention\n",
    "decoder_mask_seq0 = create_decoder_mask(max_len, device=\"cpu\", attention_mask=padding[0:1])\n",
    "decoder_mask_seq2 = create_decoder_mask(max_len, device=\"cpu\", attention_mask=padding[2:3])\n",
    "\n",
    "# Compare: no padding vs heavy padding\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[decoder_mask_seq0, decoder_mask_seq2],\n",
    "    titles=[f\"Causal + Padding (len={lengths[0]})\", f\"Causal + Padding (len={lengths[2]})\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Block Diagonal Mask\n",
    "\n",
    "Prevents attention across document boundaries in packed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence with 4 documents of different lengths\n",
    "doc_ids = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3]])\n",
    "\n",
    "block_diag = create_block_diagonal_mask(doc_ids)\n",
    "print(f\"Block diagonal mask shape: {block_diag.shape}\")\n",
    "fig = plot_attention_mask(block_diag, title=\"Block Diagonal Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combined Decoder Mask\n",
    "\n",
    "Combines causal masking with padding or block-diagonal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal + block diagonal for packed sequences\n",
    "combined = create_decoder_mask(seq_len, device=\"cpu\", attention_mask=block_diag)\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[causal, block_diag, combined],\n",
    "    titles=[\"Causal Only\", \"Block Diagonal Only\", \"Combined\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment with Different Patterns\n",
    "\n",
    "Try your own document patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Two long documents and two short ones\n",
    "doc_ids_custom = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2]])\n",
    "\n",
    "custom_mask = create_block_diagonal_mask(doc_ids_custom)\n",
    "custom_combined = create_decoder_mask(seq_len, device=\"cpu\", attention_mask=custom_mask)\n",
    "\n",
    "# Generate random attention scores (simulating QK^T output)\n",
    "torch.manual_seed(42)\n",
    "attention_scores = torch.randn(1, seq_len, seq_len)\n",
    "\n",
    "# Apply mask to attention scores (convert mask to bias)\n",
    "mask_bias = mask_to_bias(custom_combined, dtype=torch.float32)\n",
    "masked_scores = attention_scores + mask_bias\n",
    "\n",
    "# Apply softmax to get final attention weights\n",
    "attention_weights = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[attention_scores, masked_scores, attention_weights],\n",
    "    titles=[\"Raw Scores\", \"Masked Scores\", \"Attention Weights (after softmax)\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Sliding Window Attention\n",
    "\n",
    "Restricts attention to a local window of nearby tokens. Used in models like Mistral and Longformer for efficient long-context processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sliding window masks with different window sizes\n",
    "seq_len = 32\n",
    "window_sizes = [4, 8, 16]\n",
    "\n",
    "sliding_masks = []\n",
    "titles = []\n",
    "\n",
    "for ws in window_sizes:\n",
    "    mask = create_sliding_window_mask(seq_len, ws, device=\"cpu\", causal=True)\n",
    "    sliding_masks.append(mask)\n",
    "    titles.append(f\"Sliding Window (size={ws})\")\n",
    "\n",
    "# Compare different window sizes\n",
    "fig = plot_mask_comparison(masks=sliding_masks, titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal vs Bidirectional Sliding Window\n",
    "\n",
    "Causal sliding window (like Mistral) only attends to past tokens within the window.\n",
    "Bidirectional (like Longformer) attends to both past and future tokens within the window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 6\n",
    "seq_len = 20\n",
    "\n",
    "# Compare causal vs bidirectional with same window size\n",
    "full_causal = create_causal_mask(seq_len, device=\"cpu\")\n",
    "sliding_causal = create_sliding_window_mask(seq_len, window_size, device=\"cpu\", causal=True)\n",
    "sliding_bidir = create_sliding_window_mask(seq_len, window_size, device=\"cpu\", causal=False)\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[full_causal, sliding_causal, sliding_bidir],\n",
    "    titles=[\n",
    "        \"Full Causal\",\n",
    "        f\"Causal Window (size={window_size})\",\n",
    "        f\"Bidirectional Window (size={window_size})\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"Full causal attention: Each token attends to all previous tokens\")\n",
    "print(f\"Causal sliding window: Each token attends to up to {window_size} previous tokens\")\n",
    "print(f\"Bidirectional sliding window: Each token attends to Â±{window_size} nearby tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sliding Window with Padding\n",
    "\n",
    "Combining sliding window attention with padding masks - useful for batched training with variable-length sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch with different sequence lengths\n",
    "seq_len = 24\n",
    "window_size = 8\n",
    "lengths = torch.tensor([24, 16, 12])\n",
    "padding_mask = create_padding_mask(lengths, seq_len)\n",
    "\n",
    "# Create decoder masks with sliding window for different sequences\n",
    "full_len_sw = create_decoder_mask(\n",
    "    seq_len, device=\"cpu\", attention_mask=padding_mask[0:1], window_size=window_size\n",
    ")\n",
    "medium_len_sw = create_decoder_mask(\n",
    "    seq_len, device=\"cpu\", attention_mask=padding_mask[1:2], window_size=window_size\n",
    ")\n",
    "short_len_sw = create_decoder_mask(\n",
    "    seq_len, device=\"cpu\", attention_mask=padding_mask[2:3], window_size=window_size\n",
    ")\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[full_len_sw, medium_len_sw, short_len_sw],\n",
    "    titles=[\n",
    "        f\"Window {window_size}, len={lengths[0]} (no padding)\",\n",
    "        f\"Window {window_size}, len={lengths[1]} (some padding)\",\n",
    "        f\"Window {window_size}, len={lengths[2]} (heavy padding)\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Sequence lengths: {lengths.tolist()}\")\n",
    "print(\"\\nNote: Padding positions (grayed out) cannot be attended to,\")\n",
    "print(\"even if they fall within the sliding window.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zmaj-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
