{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mask Exploration\n",
    "\n",
    "This notebook demonstrates the various attention mask utilities and their visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from zmaj_lm.utils.masks import (\n",
    "    create_block_diagonal_mask,\n",
    "    create_causal_mask,\n",
    "    create_decoder_mask,\n",
    "    create_padding_mask,\n",
    "    mask_to_bias,\n",
    ")\n",
    "from zmaj_lm.utils.visualize import plot_attention_mask, plot_mask_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Causal Mask\n",
    "\n",
    "Prevents attention to future positions (lower triangular)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 16\n",
    "causal = create_causal_mask(seq_len, device=\"cpu\")\n",
    "fig = plot_attention_mask(causal, title=\"Causal Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Padding Mask\n",
    "\n",
    "Masks out padding tokens in sequences of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of 4 sequences with different lengths\n",
    "lengths = torch.tensor([16, 12, 8, 14])\n",
    "max_len = 16\n",
    "padding = create_padding_mask(lengths, max_len)\n",
    "\n",
    "print(f\"Padding mask shape: {padding.shape}\")\n",
    "print(f\"Lengths: {lengths.tolist()}\")\n",
    "\n",
    "# Combine padding with causal mask for decoder attention\n",
    "decoder_mask_seq0 = create_decoder_mask(max_len, device=\"cpu\", attention_mask=padding[0:1])\n",
    "decoder_mask_seq2 = create_decoder_mask(max_len, device=\"cpu\", attention_mask=padding[2:3])\n",
    "\n",
    "# Compare: no padding vs heavy padding\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[decoder_mask_seq0, decoder_mask_seq2],\n",
    "    titles=[f\"Causal + Padding (len={lengths[0]})\", f\"Causal + Padding (len={lengths[2]})\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Block Diagonal Mask\n",
    "\n",
    "Prevents attention across document boundaries in packed sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sequence with 4 documents of different lengths\n",
    "doc_ids = torch.tensor([[0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3]])\n",
    "\n",
    "block_diag = create_block_diagonal_mask(doc_ids)\n",
    "print(f\"Block diagonal mask shape: {block_diag.shape}\")\n",
    "fig = plot_attention_mask(block_diag, title=\"Block Diagonal Mask\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combined Decoder Mask\n",
    "\n",
    "Combines causal masking with padding or block-diagonal masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal + block diagonal for packed sequences\n",
    "combined = create_decoder_mask(seq_len, device=\"cpu\", attention_mask=block_diag)\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[causal, block_diag, combined],\n",
    "    titles=[\"Causal Only\", \"Block Diagonal Only\", \"Combined\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Experiment with Different Patterns\n",
    "\n",
    "Try your own document patterns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Two long documents and two short ones\n",
    "doc_ids_custom = torch.tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 2]])\n",
    "\n",
    "custom_mask = create_block_diagonal_mask(doc_ids_custom)\n",
    "custom_combined = create_decoder_mask(seq_len, device=\"cpu\", attention_mask=custom_mask)\n",
    "\n",
    "# Generate random attention scores (simulating QK^T output)\n",
    "torch.manual_seed(42)\n",
    "attention_scores = torch.randn(1, seq_len, seq_len)\n",
    "\n",
    "# Apply mask to attention scores (convert mask to bias)\n",
    "mask_bias = mask_to_bias(custom_combined, dtype=torch.float32)\n",
    "masked_scores = attention_scores + mask_bias\n",
    "\n",
    "# Apply softmax to get final attention weights\n",
    "attention_weights = torch.softmax(masked_scores, dim=-1)\n",
    "\n",
    "fig = plot_mask_comparison(\n",
    "    masks=[attention_scores, masked_scores, attention_weights],\n",
    "    titles=[\"Raw Scores\", \"Masked Scores\", \"Attention Weights (after softmax)\"],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zmaj-lm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
