# Configuration for training a small GPT model on TinyStories dataset
# This is a minimal example for quick experimentation and testing

model:
  vocab_size: 50257  # GPT-2 tokenizer
  max_seq_len: 512
  hidden_dim: 256
  num_layers: 6
  num_heads: 8
  mlp_dim: 1024  # 4 * hidden_dim
  dropout_rate: 0.1
  layer_norm_eps: 1.0e-5
  use_bias: true
  pos_encoding_type: learned

dataset:
  dataset_name: roneneldan/TinyStories
  dataset_config: null
  tokenizer_path: gpt2
  seq_len: 512
  batch_size: 16
  split_ratio: 0.95
  use_packing: true
  prevent_cross_doc_attention: false
  shuffle: true
  seed: 42
  cache_dir: null
  num_proc: 8
  num_workers: 4

training:
  seed: 42
  learning_rate: 3.0e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip_norm: 1.0
  batch_size: 16
  num_epochs: 3
  max_steps: null  # Set to limit total steps, overrides num_epochs
  warmup_steps: 500
  eval_every_n_steps: 1000
  checkpoint_every_n_steps: 2000
  log_every_n_steps: 100
  scheduler_type: cosine
  min_learning_rate: 1.0e-5
  use_wandb: true
  wandb_project: zmaj-lm
  wandb_run_name: tiny-stories-8L-6H-384D
  # Enable SWA/EMA for model weight averaging
  use_swa: false  # Set to true to enable
  swa_decay: null  # null for SWA (equal averaging), e.g., 0.9999 for EMA
  swa_start_step: null  # null defaults to warmup_steps
  swa_eval: true  # Use averaged model for validation
