# Configuration for training a small GPT model (GPT-neo 125M) on TinyStories dataset
# This is a minimal example for quick experimentation and testing

model:
  vocab_size: 50257
  max_seq_len: 512
  hidden_dim: 768
  num_layers: 12
  num_heads: 12
  mlp_dim: 1024  # 4 * hidden_dim
  dropout_rate: 0.0
  attention_dropout_rate: 0.0
  residual_dropout_rate: 0.0
  layer_norm_eps: 1.0e-5
  use_bias: true
  pos_encoding_type: learned

dataset:
  dataset_name: roneneldan/TinyStories
  dataset_config: null
  tokenizer_path: EleutherAI/gpt-neo-125m
  seq_len: 512
  batch_size: 16
  split_ratio: 0.95
  use_packing: true
  prevent_cross_doc_attention: false
  shuffle: true
  seed: 42
  cache_dir: null
  num_proc: 8
  num_workers: 4

training:
  seed: 42
  learning_rate: 5e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip_norm: 1.0
  batch_size: 1280 # 80 * 16 - gradient accumulation
  num_epochs: 3
  max_steps: null  # Set to limit total steps, overrides num_epochs
  warmup_steps: 500
  eval_every_n_steps: 1000
  checkpoint_every_n_steps: 2000
  log_every_n_steps: 100
  scheduler_type: constant
  min_learning_rate: 1.0e-5
  use_wandb: true
  wandb_project: zmaj-lm
  wandb_run_name: tiny-stories-8L-6H-384D
  # Enable SWA/EMA for model weight averaging
  use_swa: false  # Set to true to enable
  swa_decay: null  # null for SWA (equal averaging), e.g., 0.9999 for EMA
  swa_start_step: null  # null defaults to warmup_steps
  swa_eval: true  # Use averaged model for validation
