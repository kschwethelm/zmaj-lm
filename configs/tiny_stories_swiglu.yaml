# Configuration for training a small GPT model (GPT-neo 125M) on TinyStories dataset
# This is a minimal example for quick experimentation and testing

model:
  vocab_size: 50257
  max_seq_len: 512
  num_layers: 12
  block_config:
    hidden_dim: 768
    num_heads: 12
    mlp_dim: 683  # 2/3 or 3/4 of mlp_dim when gated
    dropout_rate: 0.0
    attention_dropout_rate: 0.0
    residual_dropout_rate: 0.0
    norm_type: rmsnorm
    use_bias: false
    pos_encoding_type: rope
    activation: swiglu

dataset:
  dataset_name: roneneldan/TinyStories
  dataset_config: null
  tokenizer_path: EleutherAI/gpt-neo-125m
  seq_len: 512
  batch_size: 16
  split_ratio: 0.95
  use_packing: true
  prevent_cross_doc_attention: false
  shuffle: true
  seed: 42
  cache_dir: null
  num_proc: 8
  num_workers: 4

training:
  seed: 42
  run_name: tiny-stories-12L-12H-768D-swiglu-rms-rope
  learning_rate: 5e-4
  weight_decay: 0.1
  beta1: 0.9
  beta2: 0.95
  grad_clip_norm: 1.0
  batch_size: 1280 # 80 * 16 - gradient accumulation
  num_epochs: 3
  max_steps: null  # Set to limit total steps, overrides num_epochs
  warmup_steps: 500
  eval_every_n_steps: 1000
  checkpoint_every_n_steps: 2000
  checkpoint_dir: checkpoints
  log_every_n_steps: 100
  scheduler_type: constant
  min_learning_rate: 1.0e-5
  use_wandb: true
  wandb_project: zmaj-lm
  # Enable SWA/EMA for model weight averaging
  use_swa: false  # Set to true to enable
  swa_decay: null  # null for SWA (equal averaging), e.g., 0.9999 for EMA
  swa_start_step: null  # null defaults to warmup_steps
  swa_eval: true  # Use averaged model for validation
